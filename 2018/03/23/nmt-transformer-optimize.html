
<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <title>Bringing TVM into TensorFlow for Optimizing Neural Machine Translation on GPU</title>
    
    <meta name="author" content="">

    <!-- Le HTML5 shim, for IE6-8 support of HTML elements -->
    <!--[if lt IE 9]>
      <script src="http://html5shim.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->

    <!-- Le styles -->
    <link href="/assets/themes/custom-twitter/css/1.4.0/bootstrap.css" rel="stylesheet">
    <link href="/assets/themes/custom-twitter/css/style.css?body=1" rel="stylesheet" type="text/css" media="all">

    <!-- Le fav and touch icons -->
  <!-- Update these with your own images
    <link rel="shortcut icon" href="images/logo/tvm-logo.png">
  <link rel="shortcut icon" href="images/logo/tvm-logo.png">
  -->
  <link href="/images/logo/tvm-logo-square.png" rel="icon" type="image/png"/>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-75982049-2"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}

    gtag('js', new Date());
    gtag('config', 'UA-75982049-2');
  </script>

</head>

  <body>
    <div class="topbar">
      <div class="fill">
        <div class="container">
          <h2 id="logo-wrap">
            <a href="/" class="nav">
              <img src="/images/logo/tvm-logo-small-black.png" width="100px">
            </a>
          </h2>
          <ul class="nav" id="nav-bar">
            
            
            



  
    
      
      
    
  
    
      
      
    
  
    
      
      
    
  
    
      
      
    
  
    
      
      
    
  
    
      
      
    
  
    
      
      	
      	<li><a href="/community">Community</a></li>
      	
      
      
    
  
    
      
      	
      	<li><a href="/download">Download</a></li>
      	
      
      
    
  
    
      
      	
      	<li><a href="/about">About</a></li>
      	
      
      
    
  
    
      
      
    
  
    
      
      	
      	<li><a href="/vta">VTA</a></li>
      	
      
      
    
  
    
      
      
      	
      	<li><a href="/blog">Blog</a></li>
      	
      
    
  




            <li> <a href="https://tvm.apache.org/docs">Docs</a></li>
            <li> <a href="https://tvmconf.org">TVM Conference</a></li>
            <li> <a href="https://github.com/apache/incubator-tvm/">Github</a></li>
            <li> <a href="/asf">ASF</a></li>
          </ul>
        </div>
      </div>
    </div>
    
<div class="container">
<div class="content">
  <div class="row">
    <div class="span14">
      <h1>Bringing TVM into TensorFlow for Optimizing Neural Machine Translation on GPU </h1>
      <p class="post-meta">
        <time datetime="2018-03-23T00:00:00-07:00" itemprop="datePublished">
          Mar 23, 2018
        </time>
        
      </p>
      <p class="post-meta">
        </p>
    </br>
    <h2 id="author">Author</h2>

<p>This is a guest blogpost contributed by Alibaba Group’s Machine Translation Platform team and PAI-Blade team</p>

<h2 id="background">Background</h2>

<p>Neural Machine Translation (NMT) is an end-to-end approach for automating translation, with the potential to overcome the weaknesses in conventional phrase-based translation systems. Recently, Alibaba Group is working on deploying NMT service for global e-commerce.</p>

<p>Currently we are exploiting Transformer [1] as the major backbone in our NMT system since it is more friendly for efficient offline training with on-par (even higher) precison against classical RNN/LSTM-based models. Although Transformer is friendly for the offline training phase as it breaks the dependencies across time steps, it is not quite efficiency for online inference. In our production environment, it has been found that the inference speed of the intial version of Transformer is around <strong>1.5X</strong> to <strong>2X</strong> slower than that of the LSTM version. Several optimizations have been undertaken to improve the inference performance, such as graph-level op fusion, loop invariant node motion [3].
One paricular challenge we observed, is that batch matmul is a major performance hot-spot in Transformer and the current implementation in cuBLAS is not well optimized.</p>

<p style="text-align: center"><img src="/images/nmt-transformer/model_arch.png" alt="image" width="40%" /></p>

<p>The results below show that TVM generated kernel (with schdule optimization) brings at least <b><em>13X</em></b> speed-up for batch matmul computation, and a futher speed up with operator fusion enabled.</p>

<p style="text-align: center"><img src="/images/nmt-transformer/batch-matmul-bar-charts.png" alt="image" width="45%" /></p>

<h2 id="batch-matmul">Batch Matmul</h2>

<h3 id="why-batch-matmul">Why batch matmul</h3>
<p>In Transformer, batch matmul is widely used in the computation of multi-head attention. Using batch matmul, multiple heads in the attention layer can run in parallel, which can help improve the computation efficiency of the hardware.</p>

<p style="text-align: center"><img src="/images/nmt-transformer/batchmatmul.png" alt="image" width="90%" /></p>

<p>We conducted a thorough profiling of the Transformer model in the inference phase, and it is shown that batch matmul computation contribute up to ~ 30% of GPU kernel execution time. Using nvprof[2] to do some first-principle analysis of cuBLAS’s batch matmul kernel，it is clearly indicated that current implementation is quite under-performing and several interesting phenomena are observed.</p>

<h3 id="what-is-batch-matmul">What is batch matmul</h3>
<p>Typically, a batch matmul computation performs the matrix-matrix multiplication over a batch of matrices. The batch is considered to be “uniform”, i.e. all instances have the same dimensions (M, N, K), leading dimensions (lda, ldb, ldc) and transpositions for their respective A, B and C matrices.</p>

<p>Batch matmul computation can be described more concretely as follows:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>void BatchedGemm(input A, input B, output C, M, N, K, batch_dimension) {
  for (int i = 0; i &lt; batch_dimension; ++i)  {
    DoGemm(A[i],B[i],C[i],M,K,N)
  }
}
</code></pre></div></div>

<h4 id="batch-matmul-shapes">Batch matmul shapes</h4>

<p>In the language translation tasks, shape of the batch matmul is significantly smaller than normal matmul computation in other workloads. The shape in Transformer is relevant to the length of input sentences and that of decoder steps. Normally, it is smaller than 30.</p>

<p>As to the batch dimension, it is a fixed number given a certain inference batch size. For instance, if 16 is used as batch size with beam size being 4, the batch dimension is 16 * 4 * #head (number of heads in multi-head attention, which is usually 8). The shape of the matrix M, K, N are within the range of  [1, max decode length] or [1, max encode length].</p>

<h3 id="performance-issue-of-cublas-batch-matmul">Performance issue of cuBLAS’ batch matmul</h3>

<p>Firstly, we make a theoretical FLOPs analysis over the batch matmul kernels. The results are quite interesting: all the batch matmul have limited computation intensity (less than 1 TFLOPs).</p>

<p>Then we profile the cuBLAS performance of batch matmul with multiple shapes through nvprof. The table below shows some of the metrics obtained on a NVIDIA M40 GPU with CUDA8.0.</p>

<table>
  <thead>
    <tr>
      <th>input shape <br /> [batch, M, N, K]</th>
      <th>kernel</th>
      <th>theoretical FLOPs</th>
      <th>nvprof observed FLOPs</th>
      <th>theoretical FLOPs / <br /> observed FLOPs</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>[512, 17, 17, 128]</td>
      <td><strong>maxwell_sgemmBatched_128x128_raggedMn_tn</strong></td>
      <td>18939904</td>
      <td>2155872256</td>
      <td>0.87%</td>
    </tr>
    <tr>
      <td>[512, 1, 17, 128]</td>
      <td><strong>maxwell_sgemmBatched_128x128_raggedMn_tn</strong></td>
      <td>1114112</td>
      <td>2155872256</td>
      <td>0.052%</td>
    </tr>
    <tr>
      <td>[512, 17, 1, 128]</td>
      <td><strong>maxwell_sgemmBatched_128x128_raggedMn_tn</strong></td>
      <td>1114112</td>
      <td>2155872256</td>
      <td>0.052%</td>
    </tr>
    <tr>
      <td>[512, 30, 30, 128]</td>
      <td><strong>maxwell_sgemmBatched_128x128_raggedMn_tn</strong></td>
      <td>58982400</td>
      <td>2155872256</td>
      <td>2.74%</td>
    </tr>
  </tbody>
</table>

<p>Even with different shapes (varing in M, N, K), all the <strong>maxwell_sgemmBatched_128x128_raggedMn_tn</strong> calls execute the same amount of FLOPs, which is much bigger than the theoretical value. It can be inferred that all these different shapes may be padded to a certain shape. Among all these shapes, even in the best case, the theoretical FLOPs is still only 2.74% of the actually executed FLOPs, <em>therefore most of the computation is quite redundant</em>. Similarly, the calls of another cuBLAS kernel <strong>maxwell_sgemmBatched_64x64_raggedMn_tn</strong> show the same phenomena.</p>

<p><b>It is obvious that cuBLAS’ batch matmul implementation is far from efficiency. Thus we use TVM to generate efficient batch matmul kernels for our NMT workloads.</b></p>

<h2 id="batch-matmul-computation">Batch matmul computation</h2>

<p>In TVM, a general batch matmul computation can be declared as:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code># computation representation
A = tvm.placeholder((batch, M, K), name='A')
B = tvm.placeholder((batch, K, N), name='B')
k = tvm.reduce_axis((0, K), 'k')
C = tvm.compute((batch, M, N),
         lambda b, y, x: tvm.sum(A[b, y, k] * B[b, k, x], axis = k),
         name = 'C')
</code></pre></div></div>

<h2 id="schedule-optimization">Schedule optimization</h2>

<p>After declaring the computation, we need to devise our own schedule carefully to squeeze performance potential.</p>

<h3 id="tuning-parameters-of-blockthread-numbers">Tuning parameters of block/thread numbers</h3>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  # thread indices
  block_y = tvm.thread_axis("blockIdx.y")
  block_x = tvm.thread_axis("blockIdx.x")
  thread_y = tvm.thread_axis((0, num_thread_y), "threadIdx.y")
  thread_x = tvm.thread_axis((0, num_thread_x), "threadIdx.x")
  thread_yz = tvm.thread_axis((0, vthread_y), "vthread", name="vy")
  thread_xz = tvm.thread_axis((0, vthread_x), "vthread", name="vx")

  # block partitioning
  BB, FF, MM, PP = s[C].op.axis
  BBFF = s[C].fuse(BB, FF)
  MMPP = s[C].fuse(MM, PP)
  by, ty_block = s[C].split(BBFF, factor = num_thread_y * vthread_y)
  bx, tx_block = s[C].split(MMPP, factor = num_thread_x * vthread_x)
  s[C].bind(by, block_y)
  s[C].bind(bx, block_x)
  vty, ty = s[C].split(ty_block, nparts = vthread_y)
  vtx, tx = s[C].split(tx_block, nparts = vthread_x)
  s[C].reorder(by, bx, vty, vtx, ty, tx)
  s[C].reorder(by, bx, ty, tx)
  s[C].bind(ty, thread_y)
  s[C].bind(tx, thread_x)
  s[C].bind(vty, thread_yz)
  s[C].bind(vtx, thread_xz)
</code></pre></div></div>
<p>We fuse the outer dimensions of the batch matmul, i.e. the BB and FF of the op’s dimension, normally known as “batch” dimension in batch matmul computation. Then we split the outer and the inner dimensions by a factor of (<code class="language-plaintext highlighter-rouge">number_thread * vthread</code>).</p>

<p>Strided pattern is not needed in batch matmul, thus the virtual thread number (<code class="language-plaintext highlighter-rouge">vthread_y</code> and <code class="language-plaintext highlighter-rouge">vthread_x</code>) are both set to 1.</p>

<h4 id="finding-the-best-combination-of-number_thread">Finding the best combination of number_thread</h4>

<p>The results below are obtained on a NVIDIA M40 GPU device with CUDA8.0.</p>

<table>
  <thead>
    <tr>
      <th>Input Shape [batch,features,M,N,K]</th>
      <th>num_thread_y, num_thread_x</th>
      <th>num_vthread_y, num_vthread_x</th>
      <th>Time(us)</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>[64,8,1,17,128]</td>
      <td>8,1</td>
      <td>32,1</td>
      <td>37.62</td>
    </tr>
    <tr>
      <td>[64,8,1,17,128]</td>
      <td>4,1</td>
      <td>32,1</td>
      <td>39.30</td>
    </tr>
    <tr>
      <td>[64,8,1,17,128]</td>
      <td>1,1</td>
      <td>32,1</td>
      <td>38.82</td>
    </tr>
    <tr>
      <td>[64,8,1,17,128]</td>
      <td>1,1</td>
      <td>256,1</td>
      <td>41.95</td>
    </tr>
    <tr>
      <td>[64,8,1,17,128]</td>
      <td>32,1</td>
      <td>1,1</td>
      <td>94.61</td>
    </tr>
  </tbody>
</table>

<p>As learned from <a href="http://tvmlang.org/2017/08/22/Optimize-Deep-Learning-GPU-Operators-with-TVM-A-Depthwise-Convolution-Example.html">past experience</a>, the method to find the best combination of <code class="language-plaintext highlighter-rouge">num_thread_y</code> and <code class="language-plaintext highlighter-rouge">num_thread_x</code> is through brute-force search. After a brute-force search, the best combination for current shape can be found, which in current computation is <code class="language-plaintext highlighter-rouge">num_thread_y</code> = 8 and <code class="language-plaintext highlighter-rouge">num_thread_x</code> = 32.</p>

<h2 id="fuse-batch-matmul-with-other-operations">Fuse batch matmul with other operations</h2>

<p>Normally, the existing “black-box” cuBLAS library calls play the role as the boundary of the normally used “op fusion” optimization tactics. However, with the generated efficient batch matmul kernel, the fusion boundary can be easily broken, more than just element-wise operations can be fused, thus futher performance improvement can be obtained.</p>

<p>It is observed from the the computation graph that a batch matmul is always followed by a <em>broadcast add</em> operation or a <em>transpose</em> operation. By fusing “add” or “transpose” operation with batch matmul, kernel launch overhead and redundant memory access time can be reduced.</p>

<p>Batch matmul and broadcast add fusion computation can be declared as follows:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code># computation representation
A = tvm.placeholder((batch_size, features, M, K), name='A')
# the shape of B is (N, K) other than (K, N) is because B is transposed is this fusion pattern
B = tvm.placeholder((batch_size, features, N, K), name='B')
ENTER = tvm.placeholder((batch_size, 1, M, N), name = 'ENTER')
k = tvm.reduce_axis((0, K), 'k')
C = tvm.compute(
           (batch_size, features, M, N),
           lambda yb, yf, m, x: tvm.sum(A[yb, yf, m, k] * B[yb, yf, x, k], axis = k),
           name = 'C')
D = topi.broadcast_add(C, ENTER)
</code></pre></div></div>

<p>Batch matmul and transpose fusion computation can be declared as:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code># computation representation
A = tvm.placeholder((batch_size, features, M, K), name='A')
B = tvm.placeholder((batch_size, features, K, N), name='B')
k = tvm.reduce_axis((0, K), 'k')
C = tvm.compute(
           (batch_size, M, features, N),
           lambda yb, m, yf, x: tvm.sum(A[yb, yf, m, k] * B[yb, yf, k, x], axis = k),
           name = 'C')
</code></pre></div></div>
<h3 id="fusion-kernel-performance">Fusion Kernel Performance</h3>

<p>The shape of [batch=64, heads=8, M=1, N=17, K=128] is chosen to elaborate the performance of the generated code. 17 is chosen as the sequence length since it is the average input length in our production scenarios.</p>

<ul>
  <li>tf-r1.4 <code class="language-plaintext highlighter-rouge">BatchMatmul</code>: 513.9 us</li>
  <li>tf-r1.4 <code class="language-plaintext highlighter-rouge">BatchMatmul</code> + <code class="language-plaintext highlighter-rouge">Transpose</code> (separate): 541.9 us</li>
  <li>TVM <code class="language-plaintext highlighter-rouge">BatchMatmul</code>: 37.62 us</li>
  <li>TVM <code class="language-plaintext highlighter-rouge">BatchMatmul</code> + <code class="language-plaintext highlighter-rouge">Transpose</code> (fused): 38.39 us</li>
</ul>

<p>The kernel fusion optimization brings a further <b><em>1.7X</em></b> speed-up.</p>

<h2 id="integration-with-tensorflow">Integration with Tensorflow</h2>

<p>The input shape of batch matmul in our workload is finite and can be enumerated easily in advance. With those pre-defined shapes, we can generate highly optimized CUDA kernel ahead of time (fixed shape computation could bring the best optimization potential). Meanwhile, a general batch matmul kernel suitable for most of the shapes will also be generated to provide a fall-back machanism for the shapes which does not have a corresponding ahead-of-time generated kernel.</p>

<p>The generated efficient kernels for specific shapes and the fall-back one are integrated into the Tensorflow framework. We develop fused ops, such as BatchMatMulTranspose or BatchMatMulAdd, to launch the specific generated kernel using TVM’s runtime API for certain input shape or invoke the fall-back kernel. A graph optimization pass is conducted to automatically replace the origin batch <em>matmul + add/transpose</em> pattern with the fused ops. Meanwhile, by combining a more aggressive graph optimization pass, we are trying to exploit TVM to generate more efficient fusion kernels for the long-tail operation patterns to further speed up the end-to-end performance.</p>

<h2 id="summary">Summary</h2>
<p>Inside Alibaba, we found that TVM is a very productive tool to develop high performance GPU kernels to meet our in-house requirements. In this blog, NMT Transformer model is taken as an example to illustrate our optimization strategy with TVM. Firstly, we locate the hot-spot of Transformer model through first-principle analysis. Then we use TVM to generate highly optimized CUDA kernel to replace cuBLAS version (<b><em>13X</em></b> speed-up is observed). Next, we leverage TVM’s kernel fusion mechanism to fuse the preceding/following operations of batch matmul to bring further performance improvement (with further <b><em>1.7X</em></b> performance improvment). The end-to-end performance improvement is <b><em>1.4X</em></b>. Based on those generated kernels a graph optimization pass is developed to replace the original computation pattern with the TVM fused kernels automatically to ensure the optimization is transparent to end users because as AI infrastructure provider we found that transparency of optimization strategy is very important to popularize its adoption. Last but not the least, all those optimizations are integrated into TensorFlow in a loosely coupled way, demonstrating a potential way for integrating TVM with different deep learning frameworks. In addition, there is an ongoing work to integrate TVM as a codegen backend for TensorFlow, we hope in the future more results could be shared with the community.</p>

<h2 id="resources">Resources</h2>
<ul>
  <li><a href="https://github.com/Orion34C/tvm-batch-matmul-example/blob/master/tvm_batch_matmul_transpose_m1_kX.py">TVM implementation of fused batch matmul + transpose computation</a></li>
</ul>

<h2 id="references">References</h2>
<p>[1] <a href="https://arxiv.org/pdf/1706.03762.pdf">Attention is All You Need</a></p>

<p>[2] <a href="https://devblogs.nvidia.com/cuda-pro-tip-nvprof-your-handy-universal-gpu-profiler/">nvprof is Your Handy Universal GPU Profiler</a></p>

<p>[3] <a href="https://github.com/tensorflow/tensorflow/pull/16306">Add Loop Invariant Node Motion Optimization in GraphOptimizer</a></p>

    </div>
  </div>
</div>
</div>


    





    <div class="container">

      <footer class="small">
        Apache TVM is an effort undergoing incubation at The Apache Software Foundation (ASF),
        sponsored by the <i>Apache Incubator</i>. Incubation is required
        of all newly accepted projects until a further review indicates that the infrastructure,
        communications, and decision making process have stabilized in a manner consistent with other
        successful ASF projects. While incubation status is not necessarily a reflection of the completeness
        or stability of the code, it does indicate that the project has yet to be fully endorsed by the ASF.

        Copyright © 2020 The Apache Software Foundation. Apache TVM, Apache,
        the Apache feather, and the Apache TVM project logo are either trademarks or registered trademarks of the Apache Software Foundation.

        See also other useful <a href="/asf" class="footer-link">ASF links</a>:
        <a href="https://www.apache.org/" class="footer-link">Apache Homepage</a>,
        <a href="https://www.apache.org/licenses/" class="footer-link">License</a>
        <a href="https://www.apache.org/foundation/sponsorship.html" class="footer-link">Sponsorship</a>,
        <a href="https://www.apache.org/security/" class="footer-link">Security</a>
        <a href="https://www.apache.org/foundation/thanks.html" class="footer-link">Thanks</a>,
        <a href="https://www.apache.org/events/current-event.html" class="footer-link">Current Event</a>

      </footer>
    </div>
  </body>
</html>


