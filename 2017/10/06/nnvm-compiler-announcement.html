
<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <title>NNVM Compiler: Open Compiler for AI Frameworks</title>
    
    <meta name="author" content="">

    <!-- Le HTML5 shim, for IE6-8 support of HTML elements -->
    <!--[if lt IE 9]>
      <script src="http://html5shim.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->

    <!-- Le styles -->
    <link href="/assets/themes/custom-twitter/css/1.4.0/bootstrap.css" rel="stylesheet">
    <link href="/assets/themes/custom-twitter/css/style.css?body=1" rel="stylesheet" type="text/css" media="all">

    <!-- Le fav and touch icons -->
  <!-- Update these with your own images
    <link rel="shortcut icon" href="images/logo/tvm-logo.png">
  <link rel="shortcut icon" href="images/logo/tvm-logo.png">
  -->
  <link href="/images/logo/tvm-logo-square.png" rel="icon" type="image/png"/>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-75982049-2"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}

    gtag('js', new Date());
    gtag('config', 'UA-75982049-2');
  </script>

</head>

  <body>
    <div class="topbar">
      <div class="fill">
        <div class="container">
          <h2 id="logo-wrap">
            <a href="/" class="nav">
              <img src="/images/logo/tvm-logo-small-black.png" width="100px">
            </a>
          </h2>
          <ul class="nav" id="nav-bar">
            
            
            



  
    
      
      
    
  
    
      
      
    
  
    
      
      
    
  
    
      
      
    
  
    
      
      
    
  
    
      
      
    
  
    
      
      	
      	<li><a href="/community">Community</a></li>
      	
      
      
    
  
    
      
      	
      	<li><a href="/download">Download</a></li>
      	
      
      
    
  
    
      
      	
      	<li><a href="/about">About</a></li>
      	
      
      
    
  
    
      
      
    
  
    
      
      	
      	<li><a href="/vta">VTA</a></li>
      	
      
      
    
  
    
      
      
      	
      	<li><a href="/blog">Blog</a></li>
      	
      
    
  




            <li> <a href="https://tvm.apache.org/docs">Docs</a></li>
            <li> <a href="https://tvmconf.org">TVM Conference</a></li>
            <li> <a href="https://github.com/apache/incubator-tvm/">Github</a></li>
            <li> <a href="/asf">ASF</a></li>
          </ul>
        </div>
      </div>
    </div>
    
<div class="container">
<div class="content">
  <div class="row">
    <div class="span14">
      <h1>NNVM Compiler: Open Compiler for AI Frameworks </h1>
      <p class="post-meta">
        <time datetime="2017-10-06T08:30:00-07:00" itemprop="datePublished">
          Oct 6, 2017
        </time>
        
      </p>
      <p class="post-meta">
        </p>
    </br>
    <p style="text-align: center">Paul G. Allen School of Computer Science &amp; Engineering, University of Washington</p>
<p style="text-align: center">Amazon Web Service AI team</p>
<p style="text-align: center">DMLC open-source community</p>

<p>Deep learning has become ubiquitous and indispensable. We are seeing a rising need for deploying deep learning workloads on many kinds of platforms such as mobile phones, GPU, IoT devices and specialized accelerators.  Last month, we announced TVM stack to close the gap between deep learning frameworks, and the performance- or efficiency-oriented hardware backends.  TVM stack makes it easy to build an end to end compilation for a deep learning framework.  However, we think it would even be better to have a unified solution that works for all frameworks.</p>

<p>Today, UW Allen school and AWS AI team, together with other contributors, are excited to announce the release of NNVM compiler, an open deep learning compiler to compile front-end framework workloads directly to hardware backends. We build it using the two-level intermediate representation(IR) in the TVM stack.
The reader is welcome to refer to the <a href="http://www.tvmlang.org/2017/08/17/tvm-release-announcement.html">original TVM announcement</a> for more technical details about TVM stack. With the help of TVM stack, NNVM compiler can:</p>

<ul>
  <li>Represent and optimize the common deep learning workloads in high level graph IR</li>
  <li>Transform the computation graph to minimize memory utilization, optimize data layout and fuse computation patterns for different hardware backends.</li>
  <li>Present an end to end compilation pipeline from front-end deep learning frameworks to bare metal hardwares.</li>
</ul>

<p style="text-align: center"><img src="/images/nnvm/nnvm_compiler_stack.png" alt="image" width="612px" /></p>

<p>The NNVM compiler can directly take models from deep learning frameworks such as Apache MXNet.
It also support model exchange formats such as ONNX and CoreML. ONNX support enables NNVM to compile deep learning models from PyTorch, Caffe2 and CNTK.
The CoreML frontend enables deployment of CoreML models to non-iOS devices.</p>

<p style="text-align: center"><img src="/images/nnvm/nnvm_compiler_code.png" alt="image" width="712px" /></p>

<h2 id="separation-of-optimization-and-deployment">Separation of Optimization and Deployment</h2>

<p style="text-align: center"><img src="/images/nnvm/nnvm_deploy.png" alt="image" width="512px" /></p>

<p>NNVM compiler applies graph level and tensor level optimizations and jointly optimize them to get the best performance. We take a different approach from existing deep learning frameworks, which packages the graph optimization with the deployment runtime.  NNVM compiler adopts the conventional wisdom from compiler to separate the optimization from the actual deployment runtime. This approach offers substantial optimization but still keeps the runtime lightweight. The compiled module only depend on a minimum TVM runtime that only takes around 300KB when deployed on a Raspberry Pi or mobile devices.</p>

<h2 id="performance">Performance</h2>

<p>NNVM compiler is still under active development, and we can expect more improvements to come, but we have started to see promising results.
We benchmarked its performance and compared it against Apache MXNet on two typical hardware configurations: ARM CPU on Raspberry PI and Nvidia GPU on AWS. Despite the radical architecture difference between these two chips, we can use the same infrastructure and only need to change the schedule for each type of hardware.</p>

<h3 id="nvidia-gpu">Nvidia GPU</h3>

<p>GPU benchmarks and schedules are contributed by Leyuan Wang (AWS/UCDavis) and Yuwei Hu (TuSimple). We compared the NNVM compiler against Apache MXNet with CUDA8 and cuDNN7 as the backend on Nvidia K80. This is a very strong baseline, as Apache MXNet turns on auto-tuning to select the best kernel from CuDNN. We also used the optimized depthwise kernel in MXNet to optimize MobileNet workload.</p>

<p style="text-align: center"><img src="/images/nnvm/nnvm_k80_result.png" alt="image" width="400px" /></p>

<p>As can be seen, NNVM compiler generate code that outperforms Apache MXNet on K80. These improvements are due to the joint graph level and kernel level optimizations. It is worth noting that NNVM compiler generates all the optimized GPU kernels on its own without relying on external libraries like CuDNN.</p>

<h3 id="raspberry-pi-3b">Raspberry Pi 3b</h3>

<p>The Rasberry Pi compilation stack is contributed by Ziheng Jiang(AWS/FDU).
We compared NNVM compiler against Apache MXNet with OpenBLAS and NNPack.
We explored the setups to get the best performance out of MXNet: we turned on Winograd convolution in the NNPACK for 3x3 convolutions, enabled multi-threading and disabled the additional scheduler thread (so all threads are used by NNPack).</p>

<p style="text-align: center"><img src="/images/nnvm/nnvm_rasp_result.png" alt="image" width="400px" /></p>

<p>As can be seen, the code generated by NNVM compiler is two times faster on ResNet18.
The gap on MobileNet is mainly due to lack of depthwise convolution in existing CPU DNN libraries. NNVM compiler takes benefit of direct generating efficient ARM code directly.</p>

<h2 id="acknowledgement">Acknowledgement</h2>
<p>This project wouldnâ€™t become possible without our early contributors in the DMLC community.
We would like to specially thank Yuwei Hu(TuSimple), Leyuan Wang(AWS/UCDavis), Joshua Z. Zhang(AWS)
and Xingjian Shi(HKUST) for their early contributions to the project. We would also like to thank all the contributors
to the TVM stack.</p>

<p>We also learnt a lot from the following projects when building NNVM Compiler.</p>
<ul>
  <li><a href="https://github.com/Theano/Theano">Theano</a>: possibly the earliest compiler for deep learning</li>
  <li><a href="https://github.com/halide/Halide">Halide</a>: TVM uses <a href="https://github.com/dmlc/HalideIR">HalideIR</a> as data structure for
arithematic simplification and low level lowering. HalideIR is derived from Halide.
We also learns from Halide when implementing the lowering pipeline in TVM.</li>
  <li><a href="https://github.com/inducer/loopy">Loopy</a>: use of integer set analysis and its loop transformation primitives.</li>
</ul>

<h2 id="links">Links</h2>
<ul>
  <li>Github page of NNVM Compiler: <a href="https://github.com/dmlc/nnvm">https://github.com/dmlc/nnvm</a></li>
  <li>Github page of TVM: <a href="https://github.com/dmlc/tvm">https://github.com/dmlc/tvm</a></li>
  <li><a href="https://news.cs.washington.edu/2017/10/06/allen-school-and-aws-team-up-on-new-nnvm-compiler-for-deep-learning-frameworks/">UW Allen school blog about NNVM compiler</a></li>
  <li><a href="https://aws.amazon.com/blogs/ai/introducing-nnvm-compiler-a-new-open-end-to-end-compiler-for-ai-frameworks/">AWS blogpost about NNVM compiler</a></li>
</ul>

    </div>
  </div>
</div>
</div>


    





    <div class="container">

      <footer class="small">
        Apache TVM is an effort undergoing incubation at The Apache Software Foundation (ASF),
        sponsored by the <i>Apache Incubator</i>. Incubation is required
        of all newly accepted projects until a further review indicates that the infrastructure,
        communications, and decision making process have stabilized in a manner consistent with other
        successful ASF projects. While incubation status is not necessarily a reflection of the completeness
        or stability of the code, it does indicate that the project has yet to be fully endorsed by the ASF.

        Copyright Â© 2020 The Apache Software Foundation. Apache TVM, Apache,
        the Apache feather, and the Apache TVM project logo are either trademarks or registered trademarks of the Apache Software Foundation.

        See also other useful <a href="/asf" class="footer-link">ASF links</a>:
        <a href="https://www.apache.org/" class="footer-link">Apache Homepage</a>,
        <a href="https://www.apache.org/licenses/" class="footer-link">License</a>
        <a href="https://www.apache.org/foundation/sponsorship.html" class="footer-link">Sponsorship</a>,
        <a href="https://www.apache.org/security/" class="footer-link">Security</a>
        <a href="https://www.apache.org/foundation/thanks.html" class="footer-link">Thanks</a>,
        <a href="https://www.apache.org/events/current-event.html" class="footer-link">Current Event</a>

      </footer>
    </div>
  </body>
</html>


